{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f9842b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import pandas as pd\n",
    "from sdg_config import *\n",
    "\n",
    "def sdg(\n",
    "    sample_size: int,\n",
    "    labels: List[str],\n",
    "    label_descriptions: str,\n",
    "    categories_types: Dict[str, List[str]],\n",
    "    use_case: str,\n",
    "    channel_type: List[str],\n",
    "    prompt_examples: str,\n",
    "    model: str,\n",
    "    max_tokens: int = 2000,\n",
    "    temperature: float = 0.7,\n",
    ") -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Generates synthetic data based on specified categories and labels.\n",
    "\n",
    "    Args:\n",
    "        sample_size (int): The number of synthetic data samples to generate.\n",
    "        labels (List[str]): The labels used to classify the synthetic data.\n",
    "        label_descriptions (str): A description of the meaning of each label.\n",
    "        categories_types (Dict[str, List[str]]): The categories and their types for data generation and diversification.\n",
    "        use_case (str): The use case of the synthetic data to provide context for the language model.\n",
    "        prompt_examples (str): The examples used in the Few-Shot or Chain-of-Thought prompting.\n",
    "        model (str): The large language model used for generating the synthetic data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str, str]: A tuple containing:\n",
    "                              - A status message indicating the save location of the synthetic data.\n",
    "                              - The path to the output CSV file.\n",
    "                              - The timestamp used in the filename.\n",
    "    \"\"\"\n",
    "    total_samples = []\n",
    "    categories = list(categories_types.keys())\n",
    "\n",
    "    for idx in range(sample_size):\n",
    "        for channel in channel_type:\n",
    "            for category in categories:\n",
    "                sub_category_list = categories_types[category]\n",
    "                for sub_category in sub_category_list:\n",
    "                    for label in labels:\n",
    "\n",
    "                        prompt = f\"\"\"You should create synthetic data for specified labels and categories.\n",
    "                        This is especially useful for {use_case}.\n",
    "                        \n",
    "                        *Label Descriptions*\n",
    "                        {label_descriptions}\n",
    "\n",
    "                        *Examples*\n",
    "                        {prompt_examples}\n",
    "\n",
    "                        ####################\n",
    "\n",
    "                        Generate one output for the classification below.\n",
    "                        You may use the examples I have provided as a guide, but you cannot simply modify or copy them.\n",
    "                        Only return the OUTPUT and REASONING. The first token in your response must be OUTPUT.\n",
    "                        Do not return the LABEL, CATEGORY, or TYPE.\n",
    "\n",
    "                        LABEL: {label}\n",
    "                        CONTACT : {channel}\n",
    "                        CATEGORY: {category}\n",
    "                        TYPE: {sub_category}\n",
    "                        OUTPUT:\n",
    "                        REASONING: \n",
    "                        \"\"\"\n",
    "\n",
    "                        messages = [\n",
    "                            {\n",
    "                                \"role\": \"system\",\n",
    "                                \"content\": f\"\"\"You are a helpful assistant designed to generate synthetic data for {use_case} with labels {labels} in categories {category} with conversation {sub_category} theme. \n",
    "                                The first token in your generated text must be OUTPUT: This must be followed by the token REASONING: as in the prompt examples.\"\"\",\n",
    "                                },\n",
    "                            {\n",
    "                                \"role\": \"user\", \n",
    "                                \"content\": prompt},\n",
    "                                ]\n",
    "\n",
    "                        batch_formatting = {\"custom_id\" : f\"{idx}_{channel}_{label}_{category}_{sub_category}\",\n",
    "                                             \"method\": \"POST\", \n",
    "                                             \"url\": \"/v1/chat/completions\",\n",
    "                                             \"body\": {\"model\": model,\n",
    "                                                      \"messages\": messages,\n",
    "                                                      \"temperature\": random.uniform(0.6, 0.9),\n",
    "                                                      #\"max_tokens\": max_tokens\n",
    "                                                      }\n",
    "                                            }\n",
    "                        total_samples.append(batch_formatting)\n",
    "\n",
    "        \n",
    "    return total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ace359",
   "metadata": {},
   "source": [
    "### Synthentic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "800d3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with Sample Size x Label x Category Type x Use Case x Contact Channel\n",
    "sdg_list_batch_api_format_list = sdg(5, labels, label_descriptions, \n",
    "                                     categories_types, use_case, \n",
    "                                     contact_chanel, prompt_examples, \n",
    "                                     'gpt-4.1')\n",
    "\n",
    "batch_api_requests_df = pd.DataFrame(sdg_list_batch_api_format_list)\n",
    "batch_api_requests_df.to_json('tesing_sdg_openai.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d35b81ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_api_requests_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0efb86",
   "metadata": {},
   "source": [
    "### Cost Estimation for OpenAI Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "692bb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prompt_ex = \"\"\"OUTPUT: ข้าพเจ้าใคร่ขอกราบเรียนมายังท่านอาจารย์ผู้ทรงคุณวุฒิ ด้วยความเคารพยิ่ง ข้าพเจ้ากำลังดำเนินงานวิจัยตามโครงการที่ได้รับมอบหมาย แต่อาจมีข้อสงสัยบางประการเกี่ยวกับแนวทางการวิเคราะห์ข้อมูล จึงขอกราบกรานความเมตตาโปรดประทานคำแนะนำอันทรงคุณค่าในการปรับปรุงและพัฒนางานวิจัยดังกล่าวต่อไปด้วยจักเป็นพระกรุณาอย่างยิ่ง\n",
    "\n",
    "REASONING: This text is highly ceremonial and formal. It utilizes very respectful pronouns like \"ข้าพเจ้า\" and addresses the professor as \"ท่านอาจารย์ผู้ทรงคุณวุฒิ.\" The sentence structure is complex and eloquent, using phrases such as \"ขอกราบกรานความเมตตาโปรดประทานคำแนะนำอันทรงคุณค่า.\" This demonstrates both a high level of politeness and reverence, typical for royal ceremonies or national events, and uses grammatically flawless language indicative of the \"พิธีการ\" level.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7db7e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def estimate_token_count(text:str, model_encoding):\n",
    "    return len(model_encoding.encode(text))\n",
    "\n",
    "def estimmate_cost_batch(tokten_count_input:int, token_count_output:int,\n",
    "                         model_cost_input:float, model_cost_output:float) -> float:\n",
    "    input_cost = (tokten_count_input/1000000) * model_cost_input\n",
    "    output_cost = (token_count_output/1000000) * model_cost_output\n",
    "    return input_cost + output_cost\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "acc_cost = 0\n",
    "total_token_count = 0\n",
    "for prompt_information in sdg_list_batch_api_format_list:\n",
    "    system_prompt = prompt_information['body']['messages'][0]['content']\n",
    "    user_prompt = prompt_information['body']['messages'][1]['content']\n",
    "    output_prompt = output_prompt_ex\n",
    "\n",
    "    system_prompt_token_count = estimate_token_count(system_prompt, enc)\n",
    "    user_prompt_token_count = estimate_token_count(user_prompt, enc)\n",
    "    output_prompt_token_count = estimate_token_count(output_prompt, enc)\n",
    "\n",
    "    cost_per_requests = estimmate_cost_batch(\n",
    "        system_prompt_token_count + user_prompt_token_count,\n",
    "        output_prompt_token_count,\n",
    "        1.00, # gpt-4.1 input cost batch\n",
    "        4.00, # gpt-4.1 output cost batch\n",
    "        )\n",
    "    \n",
    "    acc_cost += cost_per_requests\n",
    "    total_token_count += (system_prompt_token_count + user_prompt_token_count + output_prompt_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17c148b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Token Count: 5816500\n",
      "Total Cost: 7.781499999999924\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Token Count: {total_token_count}\")\n",
    "print(f\"Total Cost: {acc_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marine_sound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
