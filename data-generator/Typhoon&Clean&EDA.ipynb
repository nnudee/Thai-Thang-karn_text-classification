{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m6k5zGcbtbR"
      },
      "source": [
        "### Typhoon Synthetics Dataset\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G37klyx72-oI"
      },
      "outputs": [],
      "source": [
        "!pip install openai openpyxl -q\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"/mnt/data\")\n",
        "import sdg_config as config\n",
        "\n",
        "samples_per_combination = 10\n",
        "output_file = \"typhoon_results.xlsx\"\n",
        "model_name = \"typhoon-v2-70b-instruct\"\n",
        "\n",
        "with open(\"system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    system_prompt = f.read().strip()\n",
        "\n",
        "with open(\"user_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    user_prompt_template = f.read().strip()\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"Typhoon_API\",\n",
        "    base_url=\"https://api.opentyphoon.ai/v1\"\n",
        ")\n",
        "\n",
        "combinations = []\n",
        "for label in config.labels:\n",
        "    for contact in config.contact_chanel:\n",
        "        for category, types in config.categories_types.items():\n",
        "            for type_ in types:\n",
        "                for _ in range(samples_per_combination):\n",
        "                    combinations.append({\n",
        "                        \"label\": label,\n",
        "                        \"contact\": contact,\n",
        "                        \"category\": category,\n",
        "                        \"type\": type_\n",
        "                    })\n",
        "\n",
        "if not os.path.exists(output_file):\n",
        "    pd.DataFrame(columns=[\"label\", \"contact\", \"category\", \"type\", \"output\", \"reasoning\", \"model\"]).to_excel(output_file, index=False)\n",
        "\n",
        "def get_output_reasoning(prompt):\n",
        "    try:\n",
        "        res = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        text = res.choices[0].message.content\n",
        "        match = re.search(r\"OUTPUT:\\s*(.*?)\\s*REASONING:\\s*(.*)\", text, re.DOTALL)\n",
        "        return (match.group(1).strip(), match.group(2).strip()) if match else (\"\", \"[FAILED TO PARSE]\")\n",
        "    except Exception as e:\n",
        "        return (\"\", f\"[ERROR] {str(e)}\")\n",
        "\n",
        "total = len(combinations)\n",
        "for idx, row in enumerate(combinations):\n",
        "    label = row[\"label\"]\n",
        "    category = row[\"category\"]\n",
        "\n",
        "    # --- สุ่ม prompt_data ตาม (category, label) ---\n",
        "    key = (category, label)\n",
        "    phrases = config.prompt_data.get(key, {}).get(\"phrases\", [\n",
        "        \"ขออนุญาตสอบถามเพิ่มเติม\", \"ขอคำแนะนำ\", \"รบกวนช่วยดูหัวข้อให้หน่อย\"\n",
        "    ])\n",
        "    prompt_data = random.choice(phrases)\n",
        "\n",
        "    # --- สุ่ม pronoun ตาม label ---\n",
        "    pronouns = config.pronouns_by_level.get(label, [\"ฉัน\", \"ผม\", \"หนู\"])\n",
        "    pronoun = random.choice(pronouns)\n",
        "\n",
        "    # --- แทนที่ใน template ---\n",
        "    user_prompt = user_prompt_template.replace(\"LABEL: พิธีการ\", f\"LABEL: {label}\") \\\n",
        "                                      .replace(\"CONTACT: Email\", f\"CONTACT: {row['contact']}\") \\\n",
        "                                      .replace(\"CATEGORY: attendance_issues\", f\"CATEGORY: {category}\") \\\n",
        "                                      .replace(\"TYPE: request leave\", f\"TYPE: {row['type']}\") \\\n",
        "                                      .replace(\"OUTPUT:\", f\"PRONOUN: {pronoun}\\nPROMPT_DATA: {prompt_data}\\nOUTPUT:\")\n",
        "\n",
        "    output, reasoning = get_output_reasoning(user_prompt)\n",
        "\n",
        "    new_row = pd.DataFrame([{\n",
        "        \"label\": label,\n",
        "        \"contact\": row[\"contact\"],\n",
        "        \"category\": category,\n",
        "        \"type\": row[\"type\"],\n",
        "        \"output\": output,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"model\": model_name\n",
        "    }])\n",
        "\n",
        "    with pd.ExcelWriter(output_file, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"overlay\") as writer:\n",
        "        new_row.to_excel(writer, header=False, index=False, startrow=writer.sheets['Sheet1'].max_row)\n",
        "\n",
        "    print(f\"[{idx+1}/{total}] ✅\")\n",
        "    time.sleep(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R21baEgPMQOd"
      },
      "source": [
        "### GPT Cleaning Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PROhhqUTQ3jY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "batch_output = pd.read_json('./batch_6823ed3a1104819084d5eeb5b97556d5_output.jsonl', lines=True)\n",
        "pd.json_normalize(batch_output['response'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0k0M7lZMWgv"
      },
      "outputs": [],
      "source": [
        "batch_output = pd.read_json('./batch_6823ed3a1104819084d5eeb5b97556d5_output.jsonl', lines=True)\n",
        "body_flatten = pd.json_normalize(batch_output['response'])\n",
        "choices_flatten = pd.json_normalize(body_flatten['body.choices'])\n",
        "choices_flatten_2ndlayer = pd.json_normalize(choices_flatten[0])\n",
        "\n",
        "batch_output['output_prompt'] = choices_flatten_2ndlayer['message.content']\n",
        "batch_output.drop(columns=['error'], inplace=True)\n",
        "batch_output['body.model'] = body_flatten['body.model']\n",
        "\n",
        "batch_output['main_output'] = batch_output['output_prompt'].str.split('REASONING: ').str[0]\n",
        "batch_output['reasoning_output'] = \"REASONING: \" + batch_output['output_prompt'].str.split('REASONING: ').str[-1]\n",
        "\n",
        "batch_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vou4ywMbmRpK"
      },
      "source": [
        "Export to excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGJd5-EOk-0c"
      },
      "outputs": [],
      "source": [
        "df = batch_output.drop(['id','response','output_prompt'], axis = 1)\n",
        "\n",
        "columns = ['label', 'contact', 'category', 'type', 'output', 'reasoning','model']\n",
        "\n",
        "gpt_output = pd.DataFrame(columns=columns)\n",
        "gpt_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsGdZa7jlKyk"
      },
      "outputs": [],
      "source": [
        "split_cols = df['custom_id'].str.split('_', expand=True)\n",
        "\n",
        "split_cols.columns = ['index', 'contact', 'label', 'category', 'category1', 'type']\n",
        "\n",
        "split_cols['category'] = split_cols['category'] + ' ' + split_cols['category1']\n",
        "\n",
        "split_cols = split_cols.drop('category1', axis=1)\n",
        "\n",
        "split_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMy7Tu1VlLeF"
      },
      "outputs": [],
      "source": [
        "gpt_output['reasoning'] = df['reasoning_output']\n",
        "gpt_output['output'] = df['main_output']\n",
        "gpt_output['model'] = df['body.model']\n",
        "gpt_output['label'] = split_cols['label']\n",
        "gpt_output['contact'] = split_cols['contact']\n",
        "gpt_output['category'] = split_cols['category']\n",
        "gpt_output['type'] = split_cols['type']\n",
        "\n",
        "gpt_output['output'] = gpt_output['output'].str.replace('^OUTPUT:\\s*', '', regex=True)\n",
        "\n",
        "gpt_output['reasoning'] = gpt_output['reasoning'].str.replace('^REASONING:\\s*', '', regex=True)\n",
        "\n",
        "gpt_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXkT_fIclOtY"
      },
      "outputs": [],
      "source": [
        "gpt_output.to_excel('gpt_output.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz-0wzMPmZa0"
      },
      "source": [
        "### Final Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ovtM1VFmgEn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_list = os.listdir(\"./\")\n",
        "xlsx_file_list = [file_name for file_name in file_list if file_name.endswith(\".xlsx\")]\n",
        "\n",
        "Data = pd.DataFrame()\n",
        "for xlsx_file in xlsx_file_list:\n",
        "    Data_temp = pd.read_excel(xlsx_file)\n",
        "    Data = pd.concat([Data, Data_temp], ignore_index=True).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dlGw35d9pm6T"
      },
      "outputs": [],
      "source": [
        "Data['category'] = Data['category'].str.replace('_', ' ', regex=False)\n",
        "Data['output'] = Data['output'].str.replace(',', ' ', regex=False)\n",
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4piIEOaSrEC9"
      },
      "outputs": [],
      "source": [
        "Data.to_excel('Final_Data.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P62HVS_mD4h7"
      },
      "source": [
        "For hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "9T_KkX0UD6_M"
      },
      "outputs": [],
      "source": [
        "Data.drop(columns=['contact','type'], inplace=True)\n",
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y9C5P2u8EMm9"
      },
      "outputs": [],
      "source": [
        "Data.to_csv('Final_Data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNp7S7IXq902"
      },
      "source": [
        "### EDA Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKe9l6QPL5yu"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9ie4Ng3KwtN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "ds = load_dataset(\"nnudee/Thai-Thangkarn-sentence\", split = 'train')\n",
        "ds = ds.to_pandas()\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pGE8wtgj2vY0"
      },
      "outputs": [],
      "source": [
        "columns_to_count = [\"label\", \"contact\", \"category\", \"type\", \"model\"]\n",
        "\n",
        "for col in columns_to_count:\n",
        "    print(ds[col].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYE1z23-NSgR"
      },
      "source": [
        "Typhoon - EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IdFlgfuWNPlw"
      },
      "outputs": [],
      "source": [
        "data = ds[ds['model'] == 'typhoon-v2-70b-instruct']\n",
        "\n",
        "#Email - Chat\n",
        "\n",
        "labels = data['label'].unique()\n",
        "\n",
        "for label in labels:\n",
        "    print(f\"\\n=== Label: {label} ===\")\n",
        "\n",
        "    print(\"Email ตัวอย่าง:\")\n",
        "    email_examples = data[(data['label'] == label) & (data['contact'] == 'Email')].sample(n=5, random_state=42)\n",
        "    for text in email_examples['output']:\n",
        "        print(\"-\", text)\n",
        "\n",
        "    print(\"Chat ตัวอย่าง:\")\n",
        "    chat_examples = data[(data['label'] == label) & (data['contact'] == 'Chat')].sample(n=5, random_state=42)\n",
        "    for text in chat_examples['output']:\n",
        "        print(\"-\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HEkBgDu9uwDk"
      },
      "outputs": [],
      "source": [
        "#Each Label\n",
        "\n",
        "labels = data['label'].unique()\n",
        "\n",
        "for label in labels:\n",
        "    print(f\"\\n=== Label: {label} ===\")\n",
        "\n",
        "    for text in data[data['label'] == label ]['output'].sample(n=5, random_state=42):\n",
        "        print(\"-\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GinMrwbQASXU"
      },
      "outputs": [],
      "source": [
        "labels = ds[\"label\"].unique()\n",
        "types = ds[\"type\"].unique()\n",
        "\n",
        "for label in labels:\n",
        "    for t in types:\n",
        "        subset = ds[(ds[\"label\"] == label) & (ds[\"type\"] == t)]\n",
        "        if len(subset) >= 3:\n",
        "            examples = subset.sample(3, random_state=42).reset_index(drop=True)\n",
        "            print(f\"\\n Label: {label} |  Type: {t}\")\n",
        "            for i, row in examples.iterrows():\n",
        "                print(f\"\\n- {row['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g3zlbDYxUcY"
      },
      "source": [
        "gpt-4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6FmWD_Q0uXFt"
      },
      "outputs": [],
      "source": [
        "data = ds[ds['model'] == 'gpt-4.1-2025-04-14']\n",
        "\n",
        "#Email - Chat\n",
        "\n",
        "labels = data['label'].unique()\n",
        "\n",
        "for label in labels:\n",
        "    print(f\"\\n=== Label: {label} ===\")\n",
        "\n",
        "    print(\"Email ตัวอย่าง:\")\n",
        "    email_examples = data[(data['label'] == label) & (data['contact'] == 'Email')].sample(n=5, random_state=42)\n",
        "    for text in email_examples['output']:\n",
        "        print(\"-\", text)\n",
        "\n",
        "    print(\"Chat ตัวอย่าง:\")\n",
        "    chat_examples = data[(data['label'] == label) & (data['contact'] == 'Chat')].sample(n=5, random_state=42)\n",
        "    for text in chat_examples['output']:\n",
        "        print(\"-\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P8_tymrRwJXE"
      },
      "outputs": [],
      "source": [
        "#Each Label\n",
        "\n",
        "labels = data['label'].unique()\n",
        "\n",
        "for label in labels:\n",
        "    print(f\"\\n=== Label: {label} ===\")\n",
        "\n",
        "    for text in data[data['label'] == label ]['output'].sample(n=5, random_state=42):\n",
        "        print(\"-\", text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4csR5FvIj-V"
      },
      "source": [
        "Over all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KlETFl_xB2p"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BMwZMOxLyOfT"
      },
      "outputs": [],
      "source": [
        "from pythainlp.tokenize import word_tokenize\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KSlu_Mjm0FY6"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "sample_df = ds.sample(5, random_state=42)\n",
        "\n",
        "for i, row in sample_df.iterrows():\n",
        "    text = row[\"output\"]\n",
        "    words = word_tokenize(str(text), engine=\"newmm\")\n",
        "    print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UMcD5Fs71Bqm"
      },
      "outputs": [],
      "source": [
        "for i, row in sample_df.iterrows():\n",
        "    text = row[\"output\"]\n",
        "    words = word_tokenize(str(text), engine=\"newmm\")\n",
        "    words = [w for w in words if w.strip()]\n",
        "    print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8yVAbehyVVq"
      },
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "    words = word_tokenize(str(text), engine=\"newmm\")\n",
        "    return len(words)\n",
        "\n",
        "ds[\"thai_word_count\"] = ds[\"output\"].apply(count_words)\n",
        "\n",
        "grouped = ds.groupby([\"model\", \"label\"])[\"thai_word_count\"].mean()\n",
        "\n",
        "# ตารางเฉลี่ยตาม model และ label\n",
        "mean_table = ds.pivot_table(\n",
        "    values=\"thai_word_count\",\n",
        "    index=\"model\",\n",
        "    columns=\"label\",\n",
        "    aggfunc=\"mean\"\n",
        ").round(2)\n",
        "\n",
        "overall_avg = ds.groupby(\"label\")[\"thai_word_count\"].mean().round(2)\n",
        "\n",
        "mean_table.loc[\"average_all_models\"] = overall_avg\n",
        "\n",
        "max_table = ds.pivot_table(\n",
        "    values=\"thai_word_count\",\n",
        "    index=\"model\",\n",
        "    columns=\"label\",\n",
        "    aggfunc=\"max\"\n",
        ")\n",
        "\n",
        "min_table = ds.pivot_table(\n",
        "    values=\"thai_word_count\",\n",
        "    index=\"model\",\n",
        "    columns=\"label\",\n",
        "    aggfunc=\"min\"\n",
        ")\n",
        "\n",
        "print(\"Average:\")\n",
        "print(mean_table)\n",
        "\n",
        "print(\"\\nMax:\")\n",
        "print(max_table)\n",
        "\n",
        "print(\"\\nMin:\")\n",
        "print(min_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-V9oa-1pI4X"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def extract_english_words(text):\n",
        "    return [w.lower() for w in re.findall(r'\\b[a-zA-Z]+\\b', str(text))]\n",
        "\n",
        "ds[\"english_words\"] = ds[\"output\"].apply(extract_english_words)\n",
        "\n",
        "model_names = ds[\"model\"].unique()\n",
        "model_counters = {}\n",
        "\n",
        "for model in model_names:\n",
        "    all_words = sum(ds[ds[\"model\"] == model][\"english_words\"], [])\n",
        "    word_freq = Counter(all_words)\n",
        "    model_counters[model] = word_freq\n",
        "\n",
        "all_words_combined = sum(model_counters.values(), Counter())\n",
        "top_20_words = [word for word, _ in all_words_combined.most_common(20)]\n",
        "\n",
        "plot_data = []\n",
        "\n",
        "for word in top_20_words:\n",
        "    for model in model_names:\n",
        "        freq = model_counters[model][word]\n",
        "        plot_data.append({\n",
        "            \"word\": word.lower(),\n",
        "            \"model\": model.lower(),\n",
        "            \"frequency\": freq\n",
        "        })\n",
        "\n",
        "plot_df = pd.DataFrame(plot_data)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(data=plot_df, x=\"word\", y=\"frequency\", hue=\"model\")\n",
        "plt.title(\"word frequency comparison by model (lowercase)\", fontsize=16)\n",
        "plt.xlabel(\"word\", fontsize=14)\n",
        "plt.ylabel(\"frequency\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0fbs-W0qVCe"
      },
      "outputs": [],
      "source": [
        "contain = ds[ds[\"output\"].str.contains(\"abc\", case=False, na=False)]\n",
        "\n",
        "for i, row in contain.iterrows():\n",
        "    print(f\"ข้อความเต็ม: {row['output']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_9hdFc7rWro8"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "font_path = \"thsarabunnew-webfont.ttf\"\n",
        "fm.fontManager.addfont(font_path)  # เพิ่มฟอนต์เข้า font manager\n",
        "plt.rcParams[\"font.family\"] = \"TH Sarabun New\"\n",
        "mpl.rcParams['axes.unicode_minus'] = False  # ให้แสดงเครื่องหมายลบได้ถูกต้อง\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h6SnY8ETTjP"
      },
      "outputs": [],
      "source": [
        "from pythainlp.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "labels = ds[\"label\"].unique()\n",
        "\n",
        "for label in labels:\n",
        "    print(f\"\\n label: {label}\")\n",
        "\n",
        "    texts = ds[ds[\"label\"] == label][\"output\"].astype(str).tolist()\n",
        "    all_text = \" \".join(texts)\n",
        "\n",
        "    words = word_tokenize(all_text, engine=\"newmm\")\n",
        "    words = [w for w in words if w.strip()]\n",
        "\n",
        "    word_freq = Counter(words)\n",
        "    top_words = word_freq.most_common(20)\n",
        "\n",
        "    words, freqs = zip(*top_words)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x=list(freqs), y=list(words), palette=\"Blues_r\")\n",
        "    plt.title(f\"คำที่พบบ่อยในระดับภาษา: {label}\", fontsize=18)\n",
        "    plt.xlabel(\"จำนวนครั้ง\", fontsize=14)\n",
        "    plt.ylabel(\"คำ\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qwW-tFj7mivk",
        "8m6k5zGcbtbR",
        "hNIrqduxrx7T",
        "OM8a1sjtl-qQ",
        "05-DJqT_Aj7C"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
